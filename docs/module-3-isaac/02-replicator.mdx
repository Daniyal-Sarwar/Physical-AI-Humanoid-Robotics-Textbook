---
sidebar_position: 2
title: NVIDIA Replicator
description: Synthetic data generation for training perception models
---

# NVIDIA Replicator

## Learning Objectives

By the end of this chapter, you will be able to:

- Understand the synthetic data generation pipeline
- Configure domain randomization for robust training
- Generate labeled datasets (RGB, depth, segmentation, bounding boxes)
- Export data in common formats (COCO, KITTI, custom)
- Design randomization strategies for specific use cases

## Prerequisites

- Completed Chapter 1: Isaac Sim Introduction
- Understanding of machine learning training data
- Familiarity with computer vision concepts
- Python scripting experience

---

## Why Synthetic Data?

Real-world data collection is:
- **Expensive**: Manual labeling costs $6-10 per image
- **Time-consuming**: Months to collect diverse datasets
- **Incomplete**: Edge cases are hard to capture
- **Privacy-sensitive**: Faces, locations, proprietary items

Synthetic data solves these problems:
- **Unlimited scale**: Generate millions of samples
- **Perfect labels**: Ground truth is automatic
- **Complete coverage**: Generate any scenario
- **No privacy issues**: Fully artificial

---

## Replicator Architecture

```
┌─────────────────────────────────────────────────────┐
│                   Replicator Pipeline               │
├─────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌──────────────┐  ┌────────────┐ │
│  │   Scene     │→ │  Randomizer  │→ │  Renderer  │ │
│  │  (USD)      │  │  (Domain)    │  │  (RTX)     │ │
│  └─────────────┘  └──────────────┘  └────────────┘ │
│                                            │        │
│                                            ▼        │
│  ┌─────────────┐  ┌──────────────┐  ┌────────────┐ │
│  │   Writer    │← │  Annotator   │← │  Sensors   │ │
│  │  (COCO)     │  │  (Labels)    │  │  (Camera)  │ │
│  └─────────────┘  └──────────────┘  └────────────┘ │
└─────────────────────────────────────────────────────┘
```

---

## Basic Data Generation

```python title="basic_synthetic_data.py"
import omni.replicator.core as rep

# Create a simple scene
with rep.new_layer():
    # Camera setup
    camera = rep.create.camera(
        position=(0, 5, 10),
        look_at=(0, 0, 0)
    )
    
    # Render product (defines output)
    render_product = rep.create.render_product(camera, (1280, 720))
    
    # Create objects
    cube = rep.create.cube(
        position=(0, 0.5, 0),
        scale=1.0,
        semantics=[("class", "cube")]
    )
    
    sphere = rep.create.sphere(
        position=(2, 0.5, 0),
        scale=0.5,
        semantics=[("class", "sphere")]
    )
    
    # Ground plane
    rep.create.plane(
        scale=10,
        semantics=[("class", "floor")]
    )

# Configure writer
writer = rep.WriterRegistry.get("BasicWriter")
writer.initialize(
    output_dir="_output/basic_dataset",
    rgb=True,
    bounding_box_2d_tight=True,
    semantic_segmentation=True
)
writer.attach([render_product])

# Generate frames
rep.orchestrator.run()
```

---

## Domain Randomization

### Position Randomization

```python title="Position Randomization"
with rep.trigger.on_frame():
    # Randomize cube position
    with cube:
        rep.modify.pose(
            position=rep.distribution.uniform(
                (-3, 0.5, -3),
                (3, 0.5, 3)
            )
        )
    
    # Randomize sphere position
    with sphere:
        rep.modify.pose(
            position=rep.distribution.uniform(
                (-3, 0.5, -3),
                (3, 0.5, 3)
            ),
            rotation=rep.distribution.uniform(
                (0, 0, 0),
                (360, 360, 360)
            )
        )
```

### Material Randomization

```python title="Material Randomization"
# Create material variations
materials = [
    rep.create.material_omnipbr(
        diffuse=rep.distribution.uniform((0.2, 0.2, 0.2), (0.8, 0.8, 0.8)),
        roughness=rep.distribution.uniform(0.1, 0.9),
        metallic=rep.distribution.choice([0.0, 0.5, 1.0])
    )
    for _ in range(10)
]

with rep.trigger.on_frame():
    with cube:
        rep.randomizer.materials(materials)
```

### Lighting Randomization

```python title="Lighting Randomization"
# Create lights
lights = rep.create.light(
    light_type="distant",
    intensity=rep.distribution.uniform(500, 3000),
    color=rep.distribution.uniform((0.8, 0.8, 0.8), (1.0, 1.0, 1.0)),
    rotation=rep.distribution.uniform((-90, -180, 0), (-45, 180, 0))
)

with rep.trigger.on_frame():
    with lights:
        rep.modify.attribute(
            "intensity",
            rep.distribution.uniform(500, 3000)
        )
```

---

## Sensor Types

### RGB Camera

```python title="RGB Output"
render_product = rep.create.render_product(camera, (1920, 1080))

writer = rep.WriterRegistry.get("BasicWriter")
writer.initialize(
    output_dir="_output/rgb_dataset",
    rgb=True,
    colorize_semantic_segmentation=True
)
```

### Depth Camera

```python title="Depth Output"
writer.initialize(
    output_dir="_output/depth_dataset",
    rgb=True,
    distance_to_camera=True,  # Depth map
    distance_to_image_plane=True,  # Planar depth
    normals=True  # Surface normals
)
```

### Bounding Boxes

```python title="Detection Labels"
writer.initialize(
    output_dir="_output/detection_dataset",
    rgb=True,
    bounding_box_2d_tight=True,  # 2D tight boxes
    bounding_box_2d_loose=True,  # 2D loose boxes
    bounding_box_3d=True,  # 3D boxes
    instance_id_segmentation=True,
    semantic_segmentation=True
)
```

---

## Export Formats

### COCO Format

```python title="COCO Writer"
writer = rep.WriterRegistry.get("COCOWriter")
writer.initialize(
    output_dir="_output/coco_dataset",
    semantic_types=["class"]
)
```

### KITTI Format

```python title="KITTI Writer"
writer = rep.WriterRegistry.get("KittiWriter")
writer.initialize(
    output_dir="_output/kitti_dataset",
    semantic_filter_predicate=lambda semantic: semantic.get("class") is not None
)
```

---

## Summary

In this chapter, you learned:

- ✅ Synthetic data enables unlimited training data generation
- ✅ Replicator provides a Python API for scene randomization
- ✅ Domain randomization improves model robustness
- ✅ Multiple sensor outputs (RGB, depth, segmentation)
- ✅ Standard export formats for ML training pipelines

---

## Exercises

### Exercise 1: Object Detection Dataset (Basic)

Generate 1000 images with:
1. Random number of cubes (3-10)
2. Random colors and sizes
3. 2D bounding box annotations
4. Export in COCO format

### Exercise 2: Warehouse Scene (Intermediate)

Create a synthetic warehouse with:
1. Shelves with random products
2. Forklift robot
3. Random lighting (day/night)
4. Segmentation masks for navigation

### Exercise 3: Robotic Grasping Dataset (Advanced)

Generate training data for grasping:
1. Various household objects (mugs, bottles, tools)
2. Multiple camera viewpoints
3. 6-DOF grasp annotations
4. Depth and normal maps

---

## References

1. [Replicator Documentation](https://docs.omniverse.nvidia.com/replicator/latest/index.html)
2. [Domain Randomization Paper](https://arxiv.org/abs/1703.06907)
3. [Synthetic Data for Deep Learning](https://developer.nvidia.com/blog/training-your-ai-model-with-synthetic-data/)
