---
sidebar_position: 2
title: Vision-Language Models
description: Using VLMs for scene understanding and robot reasoning
---

# Vision-Language Models for Robotics

## Learning Objectives

By the end of this chapter, you will be able to:

- Understand the architecture of vision-language models (VLMs)
- Deploy VLMs for robotic scene understanding
- Use visual question answering for object localization
- Implement grounded language understanding
- Integrate VLM outputs with robot planning

## Prerequisites

- Completed Chapter 1: Voice to Action
- Understanding of transformer architectures
- Familiarity with computer vision concepts
- Experience with PyTorch or similar frameworks

---

## What are Vision-Language Models?

**Vision-Language Models (VLMs)** jointly understand images and text:

```
         ┌─────────────┐
Image ──▶│   Vision    │──┐
         │  Encoder    │  │    ┌─────────────┐
         └─────────────┘  ├───▶│  Language   │───▶ Response
         ┌─────────────┐  │    │   Model     │
Text  ──▶│   Text      │──┘    └─────────────┘
         │  Encoder    │
         └─────────────┘
```

### Capabilities

- **Visual Question Answering**: "What color is the cube?"
- **Object Localization**: "Where is the screwdriver?"
- **Scene Description**: "Describe what you see"
- **Spatial Reasoning**: "What is to the left of the robot?"
- **Action Grounding**: "Show me what to pick up"

### Popular VLMs

| Model | Provider | Strengths |
|-------|----------|-----------|
| GPT-4V/4o | OpenAI | General reasoning |
| Claude 3 | Anthropic | Safety, reasoning |
| Gemini | Google | Multimodal, speed |
| LLaVA | Open-source | Customizable |
| Qwen-VL | Alibaba | Multilingual |

---

## VLM Integration with ROS 2

### Camera-to-VLM Pipeline

```python title="vlm_perception_node.py"
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from cv_bridge import CvBridge
import base64
import cv2
from openai import OpenAI


class VLMPerceptionNode(Node):
    """Use VLM for scene understanding."""

    def __init__(self):
        super().__init__('vlm_perception')
        
        # OpenAI client
        self.client = OpenAI()
        self.model = "gpt-4o"
        
        # CV Bridge
        self.bridge = CvBridge()
        
        # Subscribers
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )
        
        self.query_sub = self.create_subscription(
            String,
            '/vlm/query',
            self.query_callback,
            10
        )
        
        # Publisher
        self.response_pub = self.create_publisher(
            String,
            '/vlm/response',
            10
        )
        
        # Store latest image
        self.latest_image = None
        self.latest_image_b64 = None
        
        self.get_logger().info('VLM perception ready')

    def image_callback(self, msg: Image):
        """Store latest camera image."""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
            
            # Resize for efficiency
            cv_image = cv2.resize(cv_image, (640, 480))
            
            # Encode to base64
            _, buffer = cv2.imencode('.jpg', cv_image)
            self.latest_image_b64 = base64.b64encode(buffer).decode('utf-8')
            self.latest_image = cv_image
            
        except Exception as e:
            self.get_logger().error(f'Image processing error: {e}')

    def query_callback(self, msg: String):
        """Process VLM query about current scene."""
        if self.latest_image_b64 is None:
            self.publish_response("No image available yet.")
            return
        
        query = msg.data
        self.get_logger().info(f'Query: {query}')
        
        response = self.query_vlm(query)
        self.publish_response(response)

    def query_vlm(self, query: str) -> str:
        """Send query to VLM."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """You are a robot vision system. Analyze images 
                        and answer questions to help with manipulation and navigation.
                        Be concise and specific about object locations and properties."""
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{self.latest_image_b64}"
                                }
                            },
                            {
                                "type": "text",
                                "text": query
                            }
                        ]
                    }
                ],
                max_tokens=300
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"VLM error: {e}"

    def publish_response(self, text: str):
        """Publish VLM response."""
        msg = String()
        msg.data = text
        self.response_pub.publish(msg)
        self.get_logger().info(f'Response: {text[:100]}...')
```

---

## Object Localization

### Grounded Object Detection

```python title="grounded_detection.py"
from dataclasses import dataclass
from typing import List, Tuple, Optional
import json


@dataclass
class DetectedObject:
    """Object detection with VLM grounding."""
    name: str
    bounding_box: Tuple[int, int, int, int]  # x1, y1, x2, y2
    confidence: float
    attributes: dict


class GroundedDetector:
    """Use VLM for grounded object detection."""
    
    GROUNDING_PROMPT = """Analyze this image and locate the specified object.
    Return a JSON response with:
    {
        "found": true/false,
        "objects": [
            {
                "name": "object name",
                "bounding_box": [x1, y1, x2, y2],  // normalized 0-1
                "confidence": 0.0-1.0,
                "color": "color if visible",
                "material": "material if identifiable",
                "state": "state description"
            }
        ]
    }
    
    Image dimensions are normalized to 0-1 range.
    """
    
    def __init__(self, client, model="gpt-4o"):
        self.client = client
        self.model = model
    
    def detect(
        self, 
        image_b64: str, 
        target: str
    ) -> List[DetectedObject]:
        """Detect and localize target object in image."""
        
        query = f"Find all instances of: {target}"
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.GROUNDING_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}
                        },
                        {"type": "text", "text": query}
                    ]
                }
            ],
            response_format={"type": "json_object"},
            max_tokens=500
        )
        
        result = json.loads(response.choices[0].message.content)
        
        if not result.get('found', False):
            return []
        
        return [
            DetectedObject(
                name=obj['name'],
                bounding_box=tuple(obj['bounding_box']),
                confidence=obj.get('confidence', 0.5),
                attributes={
                    'color': obj.get('color'),
                    'material': obj.get('material'),
                    'state': obj.get('state')
                }
            )
            for obj in result.get('objects', [])
        ]
```

---

## Spatial Reasoning

### Scene Graph Generation

```python title="scene_understanding.py"
class SceneAnalyzer:
    """Generate scene graphs using VLM."""
    
    SCENE_PROMPT = """Analyze this robot workspace image and create a scene graph.
    
    Output JSON:
    {
        "objects": [
            {
                "id": "obj_1",
                "type": "object type",
                "color": "color",
                "position": "description (e.g., 'center of table')",
                "reachable": true/false
            }
        ],
        "relations": [
            {
                "subject": "obj_1",
                "predicate": "relation (on, next_to, in_front_of, behind, inside)",
                "object": "obj_2 or surface name"
            }
        ],
        "workspace": {
            "surfaces": ["table", "shelf", etc],
            "obstacles": ["description of obstacles"],
            "free_space": "description of clear areas"
        }
    }
    """
    
    def analyze(self, image_b64: str) -> dict:
        """Generate scene graph from image."""
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self.SCENE_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}
                        },
                        {"type": "text", "text": "Analyze this workspace."}
                    ]
                }
            ],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
```

### Spatial Query Examples

```python title="Spatial Queries"
queries = [
    "What objects are on the table?",
    "Is there a clear path to the red cube?",
    "What is blocking the gripper's access to the bottle?",
    "Which object is closest to the robot?",
    "Are there any fragile items I should avoid?",
    "What is the safest order to stack these objects?"
]
```

---

## Integration with Planning

### VLM-Guided Task Planning

```python title="vlm_planner.py"
class VLMTaskPlanner:
    """Use VLM for high-level task planning."""
    
    PLANNING_PROMPT = """You are a robot task planner. Given the scene and goal,
    create a step-by-step plan using these primitives:
    
    - PICK(object): Pick up an object
    - PLACE(object, location): Place object at location
    - MOVE_TO(location): Navigate to location
    - OPEN(container): Open a container
    - CLOSE(container): Close a container
    - WAIT(seconds): Wait for specified time
    
    Output JSON:
    {
        "goal_understood": "restatement of goal",
        "preconditions": ["list of required conditions"],
        "plan": [
            {"step": 1, "action": "ACTION(args)", "reason": "why this step"}
        ],
        "risks": ["potential issues to monitor"],
        "success_criteria": "how to verify completion"
    }
    """
    
    def plan(self, image_b64: str, goal: str) -> dict:
        """Generate task plan from scene and goal."""
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self.PLANNING_PROMPT},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}
                        },
                        {"type": "text", "text": f"Goal: {goal}"}
                    ]
                }
            ],
            response_format={"type": "json_object"}
        )
        
        return json.loads(response.choices[0].message.content)
```

---

## Summary

In this chapter, you learned:

- ✅ VLMs enable natural language scene understanding
- ✅ Visual question answering supports robotic reasoning
- ✅ Grounded detection localizes objects from descriptions
- ✅ Scene graphs represent spatial relationships
- ✅ VLMs can guide high-level task planning

---

## Exercises

### Exercise 1: Scene Q&A (Basic)

Create a ROS 2 node that:
1. Subscribes to camera images
2. Accepts queries via service call
3. Returns VLM responses about the scene

### Exercise 2: Object Finder (Intermediate)

Build a system that:
1. Takes natural language object description
2. Returns pixel coordinates of the object
3. Draws bounding box on visualization
4. Handles "not found" cases

### Exercise 3: Autonomous Organizer (Advanced)

Create a complete system that:
1. Analyzes a messy workspace
2. Plans organization steps
3. Executes pick-and-place to organize
4. Verifies completion with VLM

---

## References

1. [GPT-4V Technical Report](https://openai.com/research/gpt-4v-system-card)
2. [LLaVA: Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)
3. [VLMs for Robotics Survey](https://arxiv.org/abs/2312.14565)
4. [Grounding DINO](https://arxiv.org/abs/2303.05499)
