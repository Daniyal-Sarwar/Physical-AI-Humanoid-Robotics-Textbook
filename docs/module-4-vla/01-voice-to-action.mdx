---
sidebar_position: 1
title: Voice to Action
description: Building natural language interfaces for robot control
---

# Voice to Action

## Learning Objectives

By the end of this chapter, you will be able to:

- Integrate speech recognition with ROS 2 systems
- Process natural language commands for robot actions
- Build intent classification pipelines for robotics
- Handle ambiguity and context in spoken commands
- Design safe and interpretable voice control systems

## Prerequisites

- Completed Module 1: ROS 2 Fundamentals
- Basic understanding of NLP concepts
- Python experience with ML libraries
- Familiarity with transformer models

---

## The Voice-to-Action Pipeline

```
┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│    Speech    │────▶│   Language   │────▶│    Action    │
│  Recognition │     │   Understanding    │     │  Execution   │
│   (ASR)      │     │   (NLU)      │     │   (Robot)    │
└──────────────┘     └──────────────┘     └──────────────┘
    "Pick up            Intent:              grasp(
     the red            PICK_UP              target=cube,
     cube"              Object: cube         color=red
                        Color: red           )
```

### Pipeline Components

1. **Automatic Speech Recognition (ASR)**: Audio → Text
2. **Natural Language Understanding (NLU)**: Text → Intent + Entities
3. **Action Mapping**: Intent → Robot Commands
4. **Execution**: Commands → Physical Actions

---

## Speech Recognition Integration

### Using Whisper with ROS 2

```python title="speech_recognition_node.py"
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import whisper
import sounddevice as sd
import numpy as np


class SpeechRecognitionNode(Node):
    """ROS 2 node for speech recognition using Whisper."""

    def __init__(self):
        super().__init__('speech_recognition')
        
        # Load Whisper model
        self.declare_parameter('model_size', 'base')
        model_size = self.get_parameter('model_size').value
        self.model = whisper.load_model(model_size)
        
        # Publisher for transcribed text
        self.publisher = self.create_publisher(
            String,
            '/speech/transcription',
            10
        )
        
        # Audio parameters
        self.sample_rate = 16000
        self.duration = 5.0  # seconds
        
        # Create timer for continuous listening
        self.timer = self.create_timer(0.1, self.listen_callback)
        self.is_recording = False
        
        self.get_logger().info('Speech recognition ready')

    def listen_callback(self):
        """Record and transcribe audio."""
        if self.is_recording:
            return
            
        self.is_recording = True
        
        try:
            # Record audio
            audio = sd.rec(
                int(self.sample_rate * self.duration),
                samplerate=self.sample_rate,
                channels=1,
                dtype=np.float32
            )
            sd.wait()
            
            # Transcribe
            result = self.model.transcribe(
                audio.flatten(),
                language='en',
                fp16=False
            )
            
            text = result['text'].strip()
            
            if text:
                msg = String()
                msg.data = text
                self.publisher.publish(msg)
                self.get_logger().info(f'Transcribed: "{text}"')
                
        except Exception as e:
            self.get_logger().error(f'Transcription error: {e}')
        finally:
            self.is_recording = False


def main(args=None):
    rclpy.init(args=args)
    node = SpeechRecognitionNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()
```

---

## Intent Classification

### Robot Command Intents

```python title="intents.py"
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, Any


class RobotIntent(Enum):
    """Robot action intents."""
    PICK_UP = "pick_up"
    PLACE = "place"
    MOVE_TO = "move_to"
    STOP = "stop"
    LOOK_AT = "look_at"
    DESCRIBE = "describe"
    NAVIGATE = "navigate"
    FOLLOW = "follow"
    UNKNOWN = "unknown"


@dataclass
class ParsedCommand:
    """Structured robot command."""
    intent: RobotIntent
    target_object: Optional[str] = None
    location: Optional[str] = None
    color: Optional[str] = None
    size: Optional[str] = None
    confidence: float = 0.0
    raw_text: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'intent': self.intent.value,
            'target_object': self.target_object,
            'location': self.location,
            'color': self.color,
            'size': self.size,
            'confidence': self.confidence,
        }
```

### LLM-Based Intent Parsing

```python title="intent_parser.py"
import json
from openai import OpenAI
from typing import Optional


class IntentParser:
    """Parse natural language commands into structured intents."""
    
    SYSTEM_PROMPT = """You are a robot command parser. Given a natural language command, 
    extract the intent and entities. Respond ONLY with valid JSON.
    
    Available intents:
    - pick_up: Pick up an object
    - place: Place an object somewhere
    - move_to: Move to a location
    - stop: Stop current action
    - look_at: Turn to look at something
    - navigate: Go to a destination
    
    Output format:
    {
        "intent": "<intent_name>",
        "target_object": "<object or null>",
        "location": "<location or null>",
        "color": "<color or null>",
        "size": "<small/medium/large or null>",
        "confidence": <0.0-1.0>
    }
    """
    
    def __init__(self, model: str = "gpt-4o-mini"):
        self.client = OpenAI()
        self.model = model
    
    def parse(self, command: str) -> Optional[ParsedCommand]:
        """Parse a natural language command."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": command}
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=200
            )
            
            result = json.loads(response.choices[0].message.content)
            
            return ParsedCommand(
                intent=RobotIntent(result.get('intent', 'unknown')),
                target_object=result.get('target_object'),
                location=result.get('location'),
                color=result.get('color'),
                size=result.get('size'),
                confidence=result.get('confidence', 0.0),
                raw_text=command
            )
            
        except Exception as e:
            print(f"Parsing error: {e}")
            return None
```

---

## Action Execution

### Command Executor Node

```python title="command_executor_node.py"
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from std_msgs.msg import String
from geometry_msgs.msg import PoseStamped

from robot_interfaces.action import PickPlace
from intent_parser import IntentParser, RobotIntent


class CommandExecutorNode(Node):
    """Execute robot commands from voice input."""

    def __init__(self):
        super().__init__('command_executor')
        
        # Intent parser
        self.parser = IntentParser()
        
        # Subscribe to transcriptions
        self.subscription = self.create_subscription(
            String,
            '/speech/transcription',
            self.command_callback,
            10
        )
        
        # Action clients
        self.pick_client = ActionClient(self, PickPlace, '/pick_place')
        
        # Status publisher
        self.status_pub = self.create_publisher(
            String,
            '/command/status',
            10
        )
        
        self.get_logger().info('Command executor ready')

    def command_callback(self, msg: String):
        """Process incoming voice command."""
        command_text = msg.data
        self.get_logger().info(f'Processing: "{command_text}"')
        
        # Parse intent
        parsed = self.parser.parse(command_text)
        
        if parsed is None or parsed.confidence < 0.5:
            self.publish_status("I didn't understand that command.")
            return
        
        # Execute based on intent
        self.execute_intent(parsed)

    def execute_intent(self, command):
        """Map intent to robot action."""
        handlers = {
            RobotIntent.PICK_UP: self.handle_pick_up,
            RobotIntent.PLACE: self.handle_place,
            RobotIntent.STOP: self.handle_stop,
            RobotIntent.MOVE_TO: self.handle_move_to,
        }
        
        handler = handlers.get(command.intent, self.handle_unknown)
        handler(command)

    def handle_pick_up(self, command):
        """Execute pick up action."""
        if not command.target_object:
            self.publish_status("What should I pick up?")
            return
        
        self.publish_status(f"Picking up {command.color or ''} {command.target_object}")
        
        # Send action goal
        goal = PickPlace.Goal()
        goal.action = "pick"
        goal.object_name = command.target_object
        goal.color_filter = command.color or ""
        
        self.pick_client.send_goal_async(goal)

    def handle_place(self, command):
        """Execute place action."""
        if not command.location:
            self.publish_status("Where should I place it?")
            return
            
        self.publish_status(f"Placing at {command.location}")
        # Implementation...

    def handle_stop(self, command):
        """Emergency stop."""
        self.publish_status("Stopping all actions!")
        # Cancel all active goals
        # Implementation...

    def handle_move_to(self, command):
        """Navigate to location."""
        self.publish_status(f"Moving to {command.location}")
        # Implementation...

    def handle_unknown(self, command):
        """Handle unknown intent."""
        self.publish_status(f"I don't know how to: {command.raw_text}")

    def publish_status(self, message: str):
        """Publish status message."""
        msg = String()
        msg.data = message
        self.status_pub.publish(msg)
        self.get_logger().info(f'Status: {message}')
```

---

## Safety Considerations

### Command Validation

```python title="Safety Checks"
class SafetyValidator:
    """Validate commands before execution."""
    
    DANGEROUS_ZONES = [
        "electrical_panel",
        "high_shelf",
        "near_human"
    ]
    
    PROTECTED_OBJECTS = [
        "human",
        "person",
        "child",
        "animal"
    ]
    
    def validate(self, command: ParsedCommand) -> tuple[bool, str]:
        """Check if command is safe to execute."""
        
        # Check target object
        if command.target_object in self.PROTECTED_OBJECTS:
            return False, f"Cannot interact with {command.target_object}"
        
        # Check location
        if command.location in self.DANGEROUS_ZONES:
            return False, f"Cannot access {command.location}"
        
        # Check for emergency keywords
        if "emergency" in command.raw_text.lower():
            return False, "Emergency detected - manual intervention required"
        
        return True, "Command validated"
```

---

## Summary

In this chapter, you learned:

- ✅ Voice-to-action pipelines convert speech to robot commands
- ✅ ASR (Whisper) provides accurate speech recognition
- ✅ LLMs can parse natural language into structured intents
- ✅ Intent-based execution maps commands to robot actions
- ✅ Safety validation is critical before execution

---

## Exercises

### Exercise 1: Basic Voice Control (Basic)

Create a system that:
1. Listens for "move forward", "turn left", "turn right", "stop"
2. Publishes appropriate Twist messages to `/cmd_vel`
3. Provides audio feedback for recognized commands

### Exercise 2: Object Manipulation (Intermediate)

Extend the system to:
1. Parse commands like "pick up the [color] [object]"
2. Query a perception system for object location
3. Execute pick-and-place actions
4. Handle "not found" scenarios gracefully

### Exercise 3: Conversational Interface (Advanced)

Build a multi-turn dialogue system:
1. Handle clarification ("Which cube?" → "The red one")
2. Maintain context across commands
3. Provide natural language status updates
4. Allow command correction ("No, the other one")

---

## References

1. [OpenAI Whisper](https://github.com/openai/whisper)
2. [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)
3. [ROS 2 Audio Common](https://github.com/ros-drivers/audio_common)
4. [Speech Recognition for Robotics](https://arxiv.org/abs/2305.18279)
